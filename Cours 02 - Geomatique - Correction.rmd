---
title: "Enjeux climatique en assurance : Introduction aux données climatiques"
author: "Thibault MONNET"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
require(pacman)
  pacman::p_load(c("data.table","dplyr","knitr","sf","viridis","leaflet","plotly","RColorBrewer","mapproj","units","mapsf","cartography","fitdistrplus","extRemes","evd","ks"), install = F, update = F, character.only = T)

```

# Dauphine 2024

## 1. Des données spatiales et temporelles

Les données climatiques peuvent être de différentes formes et selon les usages et les besoins, il faudra parfois les extrapoler avant de s'en servir. Ce cours n'abordera pas la modélisation spatio-temporelle, toutefois un exemple de données de ce type pourra être manipulé.

Avant toute chose il est nécessaire de présenter les formats et outils des données spatiales. En traitement spatial / cartographique (SIG - Système d'Information Géographique), il faut voir chaque donnée comme une couche (un calque). Ces données pouvant se superposer ou contenir des détails (attributs) permettant de les typer et les comprendre. La jointure entre 2 couches se fait via une superposition spatiale et non par une clé unique.

![](D:/Utilisateurs/s046120/Pictures/carto.jpg){height="600"}

### 1.0 Toute carte est fausse

Grande nouvelle : "LA TERRE EST RONDE !!!!"

Une carte sert à représenter des données issues d'un espace à 3 dimensions dans un espace qui n'en contient que 2. La cartographie est une science ancienne, qui a été révolutionné par la conquête de l'espace, puis par l'aire de l'informatique. Il est à présent possible de stocker de la donnée spatiale. Les sciences de l'informatique géographique sont connues sous le nom de **Géomatique**.

Vous êtes vous déjà posé la question : *le Groenland est-il si grand que cela ?*

![](https://i.insider.com/53875349ecad04d95029a00d?width=905){height="400"}

Le nom des techniques permettant de réaliser le changement de dimensions est la **projection**, il en existe plusieurs types, selon ce que l'auteur souhaite réaliser. Une carte aura donc un parti pris.

Les codes et normes des différentes projections font partis d'un système de coordonnées mondiale : [EPSG](https://epsg.io/)

Il existe 2 types de projections, celle visant à conserver les "formes", on parle de projections conformes **conformal** - elles tenteront de préserver la représentation des éléments tels qu'ils sont vus sur le globe - et celles visant à présenter une échelle constante sur la carte on parle de zones identiques ou **equal area**.

![](D:/utilisateurs/s046120/Formation/Dauphine/Cours 2024/CRS.png)

La projection mondiale la plus connue est la projection de **Mercator** (du mathématicien et cartographe éponyme), notée WGS84. Pour être exact, la projection **pseudo-Mercator** utilisée par Google Maps est très certainement la projection la plus connue.

![](https://upload.wikimedia.org/wikipedia/commons/2/24/Tissot_indicatrix_world_map_Mercator_proj.svg){height="400"}

Sur l'image précédente, on peut voir avec la taille des cercles, que cette projection va agrandir certaines zones. Elle a l'avantage de conserver les formes, c'est pour cela qu'elle est utilisée par Google notamment, un virage à 90° sera conservé, ce qui est appréciable pour du guidage GPS.

Pour comprendre un peu mieux ce que fait cette projection, on peut s'attarder sur l'image suivante. En effet, la quasi-sphère qu'est la terre va être découpée, puis les bords étirés.

![](https://upload.wikimedia.org/wikipedia/commons/9/9b/Transverse_Mercator_meridian_stripes_20deg.jpg){height="400"}

Attention, cette projection ne permet pas de conserver les distances ! Nous verrons dans le cours comment passer d'une projection à l'autre et l'intérêt de faire ces changements. On retiendra que pour calculer une distance il peut-être préférable de transformer nos données.

### 1.1 Les données vectorielles

Comme évoqué précédemment, on retrouve 2 familles de données spatiales, les données vectorielles sont les plus simples à appréhender. En effet, ces données vectorielles font références à des points, des lignes ou à des polygones.

#### 1.1.a Les **Points**

Les **points** sont identifiés par des coordonnées à 2 dimensions (longitude et latitude) et pour un système de projection donné. Il s'agit par exemple de la localisation géographique d'une adresse (on parlera de géocodage des adresses) ou encore d'un lieu d'intérêt (point de vue, emplacement d'une station météorologique ou de mesure de débit d'une rivière...). C'est le niveau le plus simple.

Pour visualiser les points, le script suivant permet de télécharger et de mettre en forme les trajectoires des ouragans dans le bassin Atlantique Nord depuis 1851.

```{r Points}
# 1 - Charger les données sur les ouragans du NHC : https://www.aoml.noaa.gov/hrd/hurdat/hurdat2.html 
source("./pgm/hurdat.R")
Atlantic <- data.table(get_hurdat("AL"))
# Description des données : https://www.aoml.noaa.gov/hrd/hurdat/hurdat2-format.pdf

# 2 - Nettoyage et mise en forme
Atlantic <- Atlantic[!is.na(DateTime)]
# On va créer des colonnes pour les couleurs à tracer, la catégorie et en fonction de l'intensité de l'évènement à chaque instant t matérialisé par un point
Atlantic <- Atlantic %>% mutate(color = as.factor(case_when(Status == 'HU' ~ 'red',
                                                 Status == 'TS' ~ 'orange',
                                                 Status == 'TD' ~ 'yellow',
                                                 TRUE ~ 'gray')), 
                               Categ = as.factor(case_when(Status == 'HU' ~ 'Hurricane',
                                                           Status == 'TS' ~ 'Tropical Storm',
                                                           Status == 'TD' ~ 'Tropical Depression',
                                                           TRUE ~ 'Other')),
                               intensity = as.factor(case_when(Status == 'HU' ~ paste0('Hurricane: ',
                                                        case_when(Atlantic$Wind >= 137 ~ 'Cat5',
                                                                  (Atlantic$Wind >= 113 & Atlantic$Wind < 137) ~ 'Cat4',
                                                                  (Atlantic$Wind >=  96 & Atlantic$Wind < 113) ~ 'Cat3',
                                                                  (Atlantic$Wind >=  83 & Atlantic$Wind <  96) ~ 'Cat2',
                                                                  (Atlantic$Wind >=  64 & Atlantic$Wind <  83) ~ 'Cat1')),
                                                     Status == 'TS' ~ 'Tropical Storm', 
                                                     Status == 'TD' ~ 'Tropical Depression',
                                                     TRUE ~ 'Other')),
                               Year = as.numeric(year(DateTime)),
                               Month = as.numeric(month(DateTime)),
                               TimePeriod10y = (paste0(as.character(round(Year-4.9,digits = -1)),"s")))

# 3 - Transformation en format spatial
# Les données de projection sont décalée sur la longitude, avant de transformer la base en données spatial il faut donc les corriger
Atlantic$Lon[Atlantic$Lon < -300] <- Atlantic$Lon[Atlantic$Lon < -300] + 360

# Rappel : https://epsg.io/4326 : projection de Mercator
AtlanticSF <- st_as_sf(Atlantic, coords = c("Lon","Lat"), crs = 4326)
# Visualiser les points 
plot(AtlanticSF["intensity"])


```

#### 1.1.b Les **lignes**

Les **lignes** sont des points reliés entre eux et ne formant pas un chemin clos, on parlerait sinon de *polygones*. Si l'on revient à nos ouragans, l'exercice suivant consiste à relier entre eux les points, par système, pour former des lignes de trajectoire.

```{r Lignes}
# 1 - Les fonctions de SF pour créer des lignes (ou des multi-lignes)
Tracks <- AtlanticSF %>% 
  group_by(Key) %>%
  summarise(do_union = FALSE) %>%
  st_cast("LINESTRING")
  
# 2 - Nettoyage : certains systèmes n'ont qu'on point et ce type de géométrie n'est pas comaptible avec les lignes... 
rmv <- Atlantic[,list(points = length(DateTime)), by = list(Key)][points == 1]

Tracks <- Tracks[!(Tracks$Key %in% rmv$Key),]
plot(st_geometry(Tracks))


```

#### 1.1.c Les **polygones**

Les **polygones** sont des lignes reliées entre elles et formant un chemin clos, cela peut correspondre notamment aux contours d'une île./
L'étape suivante consiste à récupérer et tracer la Martinique (car les données sont simple d'accès)./
Dans un second temps nous pourrons travailler sur une carte dynamique pour mieux visualiser les résultats./
Pour réaliser les cartes il est nécessaire de récupérer les fonds de cartes, certains peuvent être disponibles directement dans les données de R (et de ses librairies), à défaut des cartes par département sont disponible sur [data.gouv](https://www.data.gouv.fr/en/datasets/carte-des-departements-2-1/)

```{r polygones}
# 1 - Charger les données de la Martinique (disponible aussi dans Cartography)
mtq <- mapsf::mf_get_mtq()
plot(st_geometry(mtq), main = "Martinique")

# 2 - Leaflet et les cartes dynamiques
# Conversion de la carte dans un format compatible avec les carte leaflet (Mercator)
mtq2 <- mtq %>% sf::st_transform('+proj=longlat +datum=WGS84')
# Appel de l'application leaflet, ajout des tuiles de fonds (par défat OSM) et des polygones de la Martinique, en rouge
leaflet(mtq2, height = "500") %>% addTiles() %>% addPolygons(color = "red")

# 3 - Leaflet et lignes :
leaflet(mtq2, height = "500") %>% addTiles() %>% addPolylines(data = Tracks) %>% addPolygons(color = "red")


```

Ces cartes permettant d'empiler les données nous permettent d'envisager un comptage des systèmes ayant traversé la Martinique depuis 1851...

### 1.2 Les opérations basiques sur les données vectorielles

Une fois les données récupérées et chargées ou mises en carte, il est possible d'effectuer des opérations avec. Notamment avec les polygones, pour les regrouper, par exemple...

```{r vect_base}


# 1 - Calcul d'un centroïde de chaque polygone (peut servir pour une jointure spatiale)
mtq_c <- st_centroid(mtq)
plot(st_geometry(mtq))
plot(st_geometry(mtq_c), add=TRUE, cex=1.2, col="red", pch=20)

# 2 - calcul d'une matrice de distance 
mat <- st_distance(x=mtq_c,y=mtq_c)
mat[1:5,1:5]

# 3 - Agréger des polygones 
mtq_u <- st_union(mtq)
plot(st_geometry(mtq), col="lightblue")
plot(st_geometry(mtq_u), add=T, lwd=2, border = "red")

# 4 - Construire une zone tampon / un buffer 
mtq_b <- st_buffer(x = mtq_u, dist = 5000)
plot(st_geometry(mtq), col="lightblue")
plot(st_geometry(mtq_u), add=T, lwd=2)
plot(st_geometry(mtq_b), add=T, lwd=2, border = "red")

# 5 - Réaliser une intersection
m <- rbind(c(700015,1624212), c(700015,1641586), c(719127,1641586), c(719127,1624212), c(700015,1624212))
p <- st_sf(st_sfc(st_polygon(list(m))), crs = st_crs(mtq))
plot(st_geometry(mtq))
plot(p, border="red", lwd=2, add=T)
mtq_z <- st_intersection(x = mtq, y = p)
plot(st_geometry(mtq))
plot(st_geometry(mtq_z), col="red", border="green", add=T)

# 6 les polygones de Voronoi 
mtq_v <- st_voronoi(x = st_union(mtq_c))
mtq_v <- st_intersection(st_cast(mtq_v), st_union(mtq))
mtq_v <- st_join(x = st_sf(mtq_v), y = mtq_c, join=st_intersects)
mtq_v <- st_cast(mtq_v, "MULTIPOLYGON")
plot(st_geometry(mtq_v), col='lightblue')



```

Les **polygones de Thiessen ou de Voronoï** sont des polygones formant un pavage intégral dans lequel chaque point est entouré par un espace (une "cellule") comprenant tous les points plus proches de ce point que d'un autre. Fondamentaux en modélisation spatiale, leur usage est devenu plus courant en géographie à mesure que les outils de calcul informatique se sont démocratisés. "Dans un réseau parfait de centres équipotents, les polygones de Thiessen sont des hexagones", précise Roger Brunet (2017), ce qui renvoie à la théorie des lieux centraux.

C'est une façon de découper un territoire à partir de points. Par exemple si vous disposez de 600 points de mesure du vent en France (à un instant t), vous pouvez découper la France en polygones de Voronoi, puis calculer dans chaque polygone une estimation des vitesses du vent au sein de chaque polygone et cela en fonction des points le constituant.

```{r cartography}
# save map
# png(filename = "./data/map1.png", width = 400, height = 467, res = 100)
# set margins
# ar(mar=c(0,0,0,0))
# Countries plot
plot(st_geometry(mtq), col = "lightblue4",
     border = "lightblue3", bg = "lightblue1")
# Population plot on proportional symbols
propSymbolsLayer(x = mtq, 
                 var = "POP", 
                 legend.title.txt = "Total/npopulation")
# Title
mtext(text = "Population en Martinique", side = 3, line = -1)
# dev.off()
```

### 1.3 Les données Raster

Les données spatiales de type Raster sont assimilables à des images en terme de visualisation et à des matrices en terme de structure de données. En d'autres termes ce sont des grilles espacées spatialement et comportant à chaque noeud ou intersection une ou plusieurs informations.

La définition du raster et sa projection géographique permettant de connaître l'espacement de ce maillage./
Un raster peut se composer de plusieurs couches, les images en ont 3 pour gérer les couleurs (RGB) à 4 pour la transparence et chaque couche portera une information. Les images satellites ont régulièrement 7 bandes, pour gérer l'infrarouge et l'ultraviolet. Pour le stockage des données, on peut voir ces bandes comme une troisième dimension de notre matrice.

Certains format de données prennent également en charge la temporalité, ainsi il est possible qu'une "couche" d'un raster comporte l'axe temporel.

Un raster, en plus des données "matricielles" dispose de méta-données permettant de gérer l'aspect spacial. Parmi ces données on retrouve la projection, l'extension et la résolution.

La **projection** permet de savoir dans quel système l'on se trouve. Rien de nouveau par rapport à précédemment.

L'**extension** défini la zone géographique couverte par le raster, dans la projection donnée. Cela permet de positionner ce dernier sur une carte et surtout de réaliser des calculs spatiaux.

Enfin, la **résolution** défini la densité de point au sein de l'extension, on peut aussi voir cela comme le nombre de pixels en hauteur et largeur si l'on fait la comparaison avec les images. Meilleurs est cette dernière et plus l'image / la donnée sera précise.

![](D:/Utilisateurs/s046120/Pictures/raster_multiple_resolutions.png)

Pour plus d'information sur les raster, voir le site [neonscience.org](https://www.neonscience.org/resources/learning-hub/tutorials/raster-res-extent-pixels-r) en anglais.

Parmi les opérations "simples" que l'on peut faire sur les données spatiales, on retrouve l'estimation de la densité, ou encore le comptage au sein d'une grille (un raster donc) d'un nombre de points. Cette densité peut être brute ou lissée, par exemple via une méthode de noyaux ([KDE](https://cran.r-project.org/web/packages/ks/vignettes/kde.pdf) : Kernel Density estimation).


```{r kde}
# library(ks)
# données pour lesquelles on cherche la densité : nos points
U=Atlantic[,c("Lon","Lat")]
# retrait des lignes avec données manquante
U=U[!is.na(U$Lon),]

# Matrice de contrainte (cf. PDF associé à la méthode)
H=Hpi.diag(x=U)

# Estimation de l'estimateur de la densité par méthode des noyaux
fhat=kde(U,H,xmin=c(min(U[,1]),min(U[,2])),xmax=c(max(U[,1]),max(U[,2])))

# Carte Atlantique Nord : 
# on fixe les bornes de l'image xlim/ylim selon la longitude/lattitude 
map("world",xlim=c(-100,-50),ylim=c(10,50),col="light yellow",fill=TRUE)
# Tracé classique, avec l'option add = T (ajout de la library sf pour le spatial)
plot(fhat, display = "image", add = T)
# On ajoute les limites administrative
map("world",add = T)

# Carte Focus Antilles : idem sur une zone plus restreinte
map("world",xlim=c(-70,-50),ylim=c(10,25),col="light yellow",fill=TRUE)
plot(fhat, display = "image", add = T)
map("world",add = T)


```

On notera qu'il existe d'autres méthodes de lissage sur des données de comptage / densité. Pour cela, vous disposez de toutes les méthodologies matricielles vues dans vos cours, ainsi que tout ce qui va toucher le traitement d'image. En effet, un raster étant une matrice et une image, vous pouvez lisser le contenu d'une cellule par des approches matricielles ou par des opérations visant à réaliser un flou, à retirer un bruit, appliquer des opérations morphologiques... 

Le package [imageR](http://dahtah.github.io/imager/imager.html) permet un grand nombre de traitements. Attention, les matrices d'image et de raster ne se lisent pas dans le même sens, une prudence est donc à prévoir lors du passage d'un format à l'autre. 

Enfin, si l'on souhaite réaliser autre chose qu'un comptage, comme par exemple estimer la vitesse moyenne au sein des cellules puis la lisser, ou encore lisser un quantile extrême de vitesse de vent observée, il faudra réaliser les calculs manuellement.

La première étape consiste à définir votre grille (la base de votre raster), son extension et sa résolution, puis vous devrez réaliser une jointure entre vos données spatiales vectorielles et ces grilles. Ensuite il devient possible de réaliser des statistiques dans vos cellules et donc de définir vos couches de raster. 

Des méthodes d'interpolation spatiales vous permettent ensuite de lisser vos données. A noter que ces méthodes servent également à partir de données ponctuelles non positionnées sur une grille pour réaliser vos raster. 
L'une d'entre elle, assez efficace et présentée dans ce cours, est l'interpolation par distance inverse. Pour résumer, chaque point va donner une information sur sa zone, plus la distance entre 1 point et 1 zone à prédire est grande, moins la contribution du point y sera forte. 

Parmi les autres méthodes, on retrouve le KNN donc par plus proche voisin (distance euclidienne ou géodésique) ou encore le Krigeage qui va nécessiter d'analyser au préalable vos données, leur variance... Ces éléments ne sont pas vu dans ce cours d'introduction.

QGIS, logiciel de traitement SIG permet de manipuler ce type de données visuellement et avec une interface utilisateur. Vous pouvez vous référer à la notice sur l'interpolation qui se trouve [ici](https://docs.qgis.org/3.28/fr/docs/gentle_gis_introduction/spatial_analysis_interpolation.html), en français.


```{r raster}
# 1 : création de la grille avec st_make_grid
# 1.1 : Extension de nos données : la fonction st_bbox permet d'obtenir les bornes des données
st_bbox(AtlanticSF)
#  xmin   ymin   xmax   ymax 
# -136.9    7.0   63.0   83.0 

# la fonction st_make_grid utilise par défaut l'extension et un découpage en 10 colonnes et 10 lignes, au regard de nos données, cela semble trop large
c(diff(st_bbox(AtlanticSF)[c(1, 3)]), diff(st_bbox(AtlanticSF)[c(2, 4)]))
#  xmax  ymax 
# 199.9  76.0 

# On note que les trajectoires s'étendent sur 200° de longitude et 76° de latitudes, nous proposons donc de modifier la découpage par défaut n = c(10,10), par une approche plus fine : c(200,76), soit 1° de Longitude et 1° de Latitude (forme carrée)

# 1.2 : création de la grille
grid = st_make_grid(AtlanticSF, n = c(200,76), what = "polygons", square = TRUE) # square = FALSE permet de créer des hexagones

# 1.3 : Conversion en sf et ajout d'un ID
grid_sf = st_sf(grid) %>%
  # add grid ID
  mutate(grid_id = 1:length(lengths(grid)))

# 2 : comptage du nombre de points dans chaque cellule de la grille
# https://gis.stackexchange.com/questions/323698/counting-points-in-polygons-with-sf-package-of-r
grid_sf$n = lengths(st_intersects(grid_sf, AtlanticSF))

# Retrait des cellules sans valeurs (i.e. pas de points à l'intérieur) pour réduire la taille
count = filter(grid_sf, n > 0)

# Palette de couleur : pour représenter nos résutlats
pal <- colorNumeric(
  palette = "Reds",
  domain = range(count$n))

# Carte dynamique
leaflet(count) %>% addTiles() %>% addPolygons(color = NA, fillColor = ~pal(n), fillOpacity = 0.5)

```

On notera que le passage des points vers le raster s'est fait via la fonction "length" qui permet de réaliser un comptage, il est possible d'estimer une vitesse de vent maximale par exemple, puis ensuite de réfléchir à comment interpoler ces données sur les pojnts de la grilles sans observations.


# TP n°2 : Identification des systèmes cyclonique ayant approché les Antilles Françaises

L'objectif de ce TP est d'isoler les systèmes ayant traversé l'arc Antillais à proximité de nos îles, de les dénombrer selon leur intensité et si possible de calculer une période de retour, par intensité, de ces phénomènes.

Pour simplifier le calcul nous prendrons 3 points (centroïdes) pour représenter les îles de la Martinique, la Guadeloupe et de St Martin (et St Barthélémy). Cette étape a déja été réalisée.

Pour la première sélection, il est recommandé de comptabiliser les systèmes approchant à moins de 100km de ces points. Une fois les trajectoires sélectionnées, il faudra les catégoriser, par exemple avec le point le plus proche de nos îles pour chaque trajectoire. Enfin et si le temps le permet, une méthode permettant de passer de ce comptage à l'estimation des périodes de retour pourra être rappelé (n'hésitez pas à proposer vos idées).

## 0 - Les Antilles françaises

```{r TP0}
df <- as.data.frame(list(Ile = c("St Martin","Guadeloupe","Martinique"),
                     lon = c(-63.1,-61.5,-61), lat = c(18.05,16.25,14.6)))

```

Cette étape d'initialisation permet de charger les coordonnées des 3 îles sur lesquelles notre étude va s'attarder. Il faut pour la suite, s'assurer d'avoir chargé les données Hurdat et

## 1 - Construction de la zone de danger

La zone de danger correspond au buffer de 100 km autour de l'arc matérialisé par les 3 îles avec les coordonnées précédentes./
Au cours de cette étape, il faudra passer par une transformation de la projection pour passer dans un référentiel métrique adapté aux Antilles Françaises, [Projection Spatiale Antilles](https://epsg.io/?q=French+Antilles)./
Pour la visualisation par un "plot" traditionnel, du moment que l'on conserve la même projection entre les objets aucun problème. Pour une approche via *leaflet*, il faut repasser dans un format global, soit Mercator (4326), soit Pseudo-Mercator (ou Google - 3857).

```{r TP1}
# 1-A Spatialisation du DF
dfsf <- st_as_sf(df,coords = c("lon","lat"), crs = 4326)

# 1-B Conversion Points en ligne
Caribbeans <- dfsf %>% summarise(do_union = FALSE) %>%
  st_cast("LINESTRING")

# 1-C Transformation en projection métrique : 
# Les projections disponibles aux Antilles https://epsg.io/?q=French+Antilles
# Le choix : https://epsg.io/5490 (on notera l'unité métrique)
CaribbeansM <- st_transform(Caribbeans, '+init=epsg:5490')

# 1-D Ajout d'un buffer de 100km soit 100000m (attention aux unités)
DangerZoneM <- st_buffer(x = CaribbeansM, dist = 100000)
DangerZone <- st_transform(DangerZoneM, '+init=epsg:4326')
# NB : pour travailler les données avec des calculs en km nous devons convertir et obtenons des format de projection métrique.
# Pour afficher dans un outil dynamique (type leaflet), nous devons retourner dans un format compatible (de type Mercator)

# 1- Visualisation : essayez addProviderTiles pour passer sur d'autres cartes "Esri.NatGeoWorldMap"
# Les cartes disponibles : http://leaflet-extras.github.io/leaflet-providers/preview/
leaflet(height = "500") %>% addProviderTiles("Esri.NatGeoWorldMap") %>% addPolylines(data = Caribbeans, color = "darkBlue") %>% addPolygons(data = DangerZone, color = "red")


```

## 2 - Croisement des lignes de trajectoire des ouragans avec la zone de danger

Une fois la zone "à risque" définie, on va sélectionner les trajectoires qui la traverse. Pour cette étape, on utilisera une intersection. Toutefois, au lieu d'utiliser st_intersection comme dans l'exemple, il est préconisé d'utiliser la fonction *st_intersects*.

```{r TP2}
# Superposition spatiale : st_intersects

# Les objets sf doivent avoir la même projection pour ce traitement
# Contrôles 
st_crs(Tracks) == st_crs(DangerZone)
# TRUE !! on peut continuer 
# on pourrait mettre une condition pour la suite du traitement : if(st_crs(Tracks) == st_crs(DangerZone)){...}
# Avec st_intersects on extrait les trajectories qui coupent la zone de danger
over <- unlist(st_intersects(DangerZone, Tracks))

# On peut enrichir la base Tracks ou les isoler
# Extraction des lignes dangereuses
DangerousTracks <- Tracks[over,]

# Ajout d'une variable booléenne Danger : facultatif, permet de colorer différemment les trajectoires depuis la même source
Tracks$Danger <- F
Tracks[over,]$Danger <- T

# Visualisation et ajout dans leaflet de groupes pour filtrer l'affichage
leaflet(Caribbeans, height = "500") %>% addTiles() %>% addPolylines(data = Tracks, group = "All") %>% addPolylines(data = DangerousTracks, group = "Danger", color = "orange") %>% addLayersControl(overlayGroups =c("All"), options = layersControlOptions(collapsed=FALSE)) %>%  addPolygons(data = DangerZone, color = "red") %>% addPolylines(color = "blue")


```

## 3 - Enrichir les traces de l'information (intensité, vitesse de vent...) contenue dans le point de la trace le plus proche des îles

Cette étape consiste à définir le point le plus proche de nos îles pour chacune des 174 trajectoires dangereuses. Il est possible d'utiliser des fonctions existantes ou, comme la correction le propose, de faire le calcul soit-même pour bien maîtriser ce qui est fait./
Pour faire les choses "parfaitement", il faudrait chercher le point le plus proche à la fois spatialement et temporellement. En effet, ce qui nous intéresse c'est la force du vent au moment de l'impact, dit autrement, l'information connue juste avant que la trajectoire traverse la zone de danger. Typiquement dans un devoir maison, expliquer ce qui serait le mieux à faire et proposer une solution dégradée que vous aurez su faire sera important.

Pour cette étape, qui peut s'avérer longue (calcul des distances entre un grand nombre de point), une première précaution vise donc à ne travailler que sur les 174 trajectoire dangereuse. La seconde précaution va elle être de se contenter de filtrer les points de ces trajectoires à l'intérieur d'une zone d'intérêt (un peu comme lors de l'exemple d'intersection). On pourra définir la zone d'intérêt comme les coordonnées min et max des îles élargie d'1 ou 2 degré(s).

```{r TP3}

# 3.1 - Réduction de la zone de calcul (pour réduire les points et accélérer le temps de calcul)
ManualBuffer <- 1
Filter <- st_as_sfc(st_bbox(c(xmin = floor(min(df$lon)-ManualBuffer), xmax = ceiling(max(df$lon)+ManualBuffer), ymax = floor(min(df$lat)-ManualBuffer), ymin = ceiling(max(df$lat)+ManualBuffer)), crs = st_crs(4326)))

# 3.2 - Application du filtre sur les données "points" Atlantic pour les systèmes identifié comme dangereux
NearCaribbeans <- st_intersection(x = AtlanticSF[AtlanticSF$Key %in% DangerousTracks$Key,], y = Filter)

# 3.3 - Matrice de distance
DistMat <- data.table(Key = NearCaribbeans$Key, DateTime = NearCaribbeans$DateTime,  st_distance(x=NearCaribbeans, y=dfsf))

# 3.4 - Identification de la distance la plus courte entre 1 système et l'une des 3 îles
DistMat$nearest <- pmin(DistMat$V1,DistMat$V2,DistMat$V3)

# 3.5 - Identification du point le plus proche
Nearest <- DistMat[,list(nearest = min(nearest)), by = "Key"] 
Nearest <- merge(Nearest, DistMat[,c("Key","nearest","DateTime")], by = c("Key","nearest"))


# 3.6 - Ajout des données du point le plus proche des îles à la trace pour la catégoriser
DangerousTracksDT <- merge(DangerousTracks, merge(st_drop_geometry(NearCaribbeans), Nearest, by=c("Key","DateTime"), all.y = T)[,c("Key","Name","DateTime","Record","Status","Wind","Pressure","Categ","intensity")], by = "Key")

DT <- data.table(st_drop_geometry(DangerousTracksDT))
kable(DT[,list(N_systeme = length(Key)), by = list(Categ)])
```

On pourra noter l'utilisation de la fonction *st_drop_geometry* qui permet de passer d'un format spatial à un format classique et donc de faire la jointure traditionnelle et non spatiale.

Comme évoqué en préambule de cette étape, plutôt que de faire tous ces calculs, il est possible d'extraire spatialement directement le point le plus proche pour chaque traces avec la commande *st_nearest_points*, toutefois il faut penser à appliquer cette fonction par système et pas sur toute la base (avec un **lapply** par exemple)

## 4 - Statistiques et étude de la période de retour

L'objectif final est d'établir une période de retour, soit l'inverse de la probabilité de survenance d'un évènement. Si l'on s'attarde sur la vitesse du vent, la première étape serait donc de tenter de modéliser la distribution de la vitesse du vent à partir des 174 points disponibles. Méfiance toutefois avec cette approche, si plusieurs évènements ont lieu certaines années, l'objectif étant de mesurer plutôt un vent maximal annuel, il faudra retravailler les données. Sur cette même idée, si nous n'avons pas de mesure sur une (ou plusieurs) année(s), notre série est tronquée, cela pourra soit être compensé par l'ajout de donnée (à 0?) soit par l'application de la troncature adéquate lors de l'ajustement de la loi.

```{r units}

# Rappel sur les unités :
unites <- data.table(valid_udunits())
# unites[grepl("knot",unites$name_singular)]
# unites[grepl("nautic",unites$name_singular)]
# 1 noeud vaut donc 1852m / heure ou 1.852 km/h
max(DT$Wind) * 1.852


```

1 noeud vaut donc 1852m / heure ou 1.852 km/h et la vitesse maximale mesurée à l'approche des côtes était de 287 km/h en vent moyen...

### 4.1 - Analyse annuelle

```{r TP4.1, echo=FALSE, message=FALSE, warning=FALSE}

DT[,list(N_systeme = length(Key)), by = list(Categ)]

# 4.1 - Analyse par catégorie (intensité)
DT$Year <- year(DT$DateTime)

ggplot(DT[, list(N = length(Key)), by = list(Year,Categ)], 
       aes(fill=factor(Categ,levels=c("Other", "Tropical Depression", "Tropical Storm","Hurricane")), y=N, x=Year)) + 
  geom_bar(position='stack', stat='identity') + 
  labs(x='Année', y='Systèmes', title='Système par an et par catégorie') +
  scale_fill_manual('Categ', values=c('gray','yellow','orange','red')) +
  theme_classic()

# 4.2 - Analyse temporelle

```

On voir bien que certaines années aucun système ne traverse la zone. De plus, les *Dépressions Tropicales* ne sont répertoriées, au mieux, qu'à partir des année 1930, et les évènements *autres* à partir des années 2000.../
Par conséquent, notre historique n'est pas complet, nous pouvons soit réaliser notre modèle à partir des années 2000 (attention peu de profondeur) soit retirer les petits évènements, notre loi sera donc tronquée à gauche./
Une alternative consisterait à tenter de modéliser un max annuel et ne retenir pour chaque année 1 seul évènement. Dans cette hypothèse, les années sans évènement, la vitesse maximale du vent retenue sera de 30 noeuds (soit 55 km/h).

### 4.2 - Alternative "Simple"

Dans cette section nous allons établir une série temporelle contenant l'évènement annuel ayant les caractéristiques de vitesse de l'évènement le plus fort. Pour les années sans évènement nous modéliserons notre loi en partant du principe que nous observons des données tronquées (seuil 35 noeuds).

```{r TP4.2.1}

# Base de Vent max par an depuis 1851 
DT2 <- DT[,list(Wind = max(Wind)), by = Year]

# Distribution de la vitesse du vent max annuel 
descdist(DT2$Wind, boot = 100)

maxi <- max(DT2$Wind, 180)
mini <- min(DT2$Wind, 0)
xscaled <- (DT2$Wind-mini)/maxi
fit.beta <- fitdist(xscaled, "beta",  method = "mse")
plot(fit.beta); title("BETA")
summary(fit.beta)

fit.gev <- extRemes::fevd(DT2$Wind,type="GEV",threshold=35)
plot(fit.gev); title("GEV")
summary(fit.gev)

fit.gpd <- extRemes::fevd(DT2$Wind,type="GP",threshold=35)
plot(fit.gpd); title("GPD")
summary(fit.gpd)
```

Le choix de la loi à retenir s'effectue entre la loi Beta, une GEV et la Distribution de Pareto Généralisée avec un seuil de 35 noeuds.

```{r TP4.2.2}
ReturnPeriod <- data.table(rbind(
  data.frame(law = "Beta", PdR_100ans = round(maxi * qbeta(p = 0.99, fit.beta$estimate[1], fit.beta$estimate[2]) + mini, digits = 2),
             PdR_200ans = round(maxi * qbeta(p = 0.995, fit.beta$estimate[1], fit.beta$estimate[2]) + mini, digits = 2),
             PdR_max = round(1 / (1 - pbeta(q = (max(DT$Wind)-mini)/maxi, fit.beta$estimate[1], fit.beta$estimate[2])), digits = 0),
             PdR_cat4 = round(1 / (1 - pbeta(q = (113-mini)/maxi, fit.beta$estimate[1], fit.beta$estimate[2])), digits = 0),
             PdR_cat5 = round(1 / (1 - pbeta(q = (137-mini)/maxi, fit.beta$estimate[1], fit.beta$estimate[2])), digits = 0)),
  data.frame(law = "GEV", PdR_100ans = fExtremes::qgev(0.99, mu = fit.gev$results$par[1], beta = fit.gev$results$par[2], xi = fit.gev$results$par[3])[1],
             PdR_200ans = fExtremes::qgev(0.995, mu = fit.gev$results$par[1], beta = fit.gev$results$par[2], xi = fit.gev$results$par[3])[1],
             PdR_max = round(1 / (1 - fExtremes::pgev(max(DT$Wind), mu = fit.gev$results$par[1], beta = fit.gev$results$par[2], xi = fit.gev$results$par[3])),digits = 0),
             PdR_cat4 = round(1 / (1 - fExtremes::pgev(113, mu = fit.gev$results$par[1], beta = fit.gev$results$par[2], xi = fit.gev$results$par[3])),digits = 0),
             PdR_cat5 = round(1 / (1 - fExtremes::pgev(137, mu = fit.gev$results$par[1], beta = fit.gev$results$par[2], xi = fit.gev$results$par[3])),digits = 0)),
  data.frame(law = "GPD", PdR_100ans = evd::qgpd(0.99, loc = 35, scale = fit.gpd$results$par[1], shape = fit.gpd$results$par[2]),
             PdR_200ans = evd::qgpd(0.995, loc = 35, scale = fit.gpd$results$par[1], shape = fit.gpd$results$par[2]),
             PdR_max = round(1 / (1 - evd::pgpd(max(DT$Wind), loc = 35, scale = fit.gpd$results$par[1], shape = fit.gpd$results$par[2])),digits = 0),
             PdR_cat4 = round(1 / (1 - evd::pgpd(113, loc = 35, scale = fit.gpd$results$par[1], shape = fit.gpd$results$par[2])),digits = 0),
             PdR_cat5 = round(1 / (1 - evd::pgpd(137, loc = 35, scale = fit.gpd$results$par[1], shape = fit.gpd$results$par[2])),digits = 0))))

ReturnPeriod
  
```

A l'étude des courbes, la GPD a un inconvénient majeur, elle a du mal à excéder la vitesse maximale observée... Les autres méthodes permettent de calculer des périodes de retour pour des évènements de catégorie 4 et 5, tout comment les vents observables tous les 100 ou 200 ans...

Etant donné que l'on a parfois observé plusieurs évènements d'une intensité forte la même année, évènement n'étant pas le plus fort de l'année mais largement plus fort que les évènements d'autres années, il convient de rester prudent sur cette première approche. Elle permet toutefois de proposer une première réponse à la question du TP.

Si l'on croise ces résultats avec d'autres études, notamment celle réalisée en 2020 par la CCR, on aurait tendance à retenir la GEV, même si cette dernière semble avoir des périodes de retour plus faibles que celle de l'étude de référence. De plus, les vents annoncés pour des périodes de retour de 100 ans ou 200 ans sont supérieurs à toutes les vitesses enregistrée dans la base complète (Atlantic : Vmax = 165 kn = 305 km/h).

### 4.3 - Approche plus complète

Prenons le temps d'étudier l'intégralité des données et non plus le maximum annuel.

```{r TP4.3.1}

# Densité de tout le set
ggplot(DT, aes(x = Wind)) + geom_density() + theme_classic()

# Densité de tout le set en fonction de l'intensité (non exploitable)
ggplot(DT, aes(x = Wind)) +
  geom_density(aes(color = Categ) ) + theme_classic()

# Sélection des évènements à partir de l'année 2000 (set comparable)
ggplot(DT[Year >= 2000], aes(x = Wind)) + geom_density() + theme_classic() +
  labs(title='Post 2000 - périmètre comparable')

# Sélection des évènements de type Tropical Storm ou Hurricane (set comparable)
ggplot(DT[Categ %in% c("Tropical Storm","Hurricane")], aes(x = Wind)) + geom_density() + theme_classic() +
  labs(title='Tropical Storm & Hurricane')

```

Nous rappellerons qu'en 4.1 nous avons identifié que l'historique ancien n'était pas à périmètre identique de l'historique récent. Ces graphiques montrent le lien immédiat entre la catégorie (fonction du vent) et la vitesse du vent. Concernant les données récentes, la distribution s'écarte assez fortement de la distribution complète (et tronquées), ce qui semble moins le cas de la distribution des évènements "majeurs" (type "Tropical Storm" **TS** et "Hurricane" **HU**).

**L'idée ici serait de construire une approche Bayésienne du système.**

La difficulté de l'exercice est de mettre sous forme d'équation notre problématique./
Posons X la vitesse de vent maximale au passage dans la zone de danger et Y la catégorie de l'évènement.

$P(X|Y)=/frac{P(Y|X).P(X)}{P(Y)}$

Nous pouvons donc à ajuster $P(X /ge x|Y=y)$ la probabilité que X dépasse x noeuds, sachant que les évènements sont de type Tempête Tropicale ou Ouragan./
Nous cherchons cependant à ajuster $P(X/ge x)$ quelle que soit la catégorie y de Y.../
La probabilité $P(Y=y)$ peut s'ajuster sur un set plus récent et complet et se traduire comme la probabilité qu'un évènement depuis 2000 soit de type Tempête Tropicale ou Ouragan.

La dernière partie de l'équation, soit $P(Y=y|X>x)$ est elle instantanée puisque par construction, la catégorie est définie par la vitesse du vent. Ainsi, dès que la vitesse atteint 34 noeuds le système peut être considéré comme Tempête Tropicale ou Ouragan.

On pourra alors définir $P(X/ge x) = P(Y=y) * /frac{P(X/ge x|Y=y)}{P(Y=y|X/ge x)}$ avec pour x /> 34 $P(Y=y|X/ge x) = 1$.

**A ce stade nous avons 2 options**, la première consiste à estimer empiriquement $P(Y=y)$ de sorte que sur l'ensemble des évènements observés depuis l'an 2000, la probabilité d'observer un évènement de type **TS** ou **HU** soit le nombre de ces évènements rapporté au nombre total d'évènements observé sur la période.

```{r TP4.3.2}

# P(Y) : période d'analyse 2000 - 2021
Y_est <- DT[Year >= 2000 & Categ %in% c("Tropical Storm","Hurricane"), list(N = length(Key))] / DT[Year >= 2000, list(N = length(Key))]

Y <- merge(DT[Year >= 2000 & Categ %in% c("Tropical Storm","Hurricane"), list(N_y = length(Key)),by = list(Year)], DT[Year >= 2000, list(N = length(Key)),by = list(Year)], by = "Year")
Y$p <- Y$N_y/Y$N

ggplot(Y, aes(x = p)) +
  geom_density() + theme_classic()
descdist(Y$p, boot = 100)

fit.beta <- fitdist(Y$p, "beta", method = "mme");summary(fit.beta);plot(fit.beta)


```

En posant, comme dans le support de cours, **Y/~B(n,p)** avec n le nombre d'évènement par an (que l'on notera Z) et p la probabilité qu'un évènement n dépasse le seuil d'une Tempête Tropicale (**TS**), nous obtenons **p = 70%**

Si l'on s'attarde sur les chiffres, on notera que si 1 seul système traverse la zone il a historiquement 100% de probabilité d'être de type **TS** ou **HU**, cette probabilité est presque la même pour les années avec 3 évènements... Il reste les années à 2 exercices oÃÂ¹ 3 fois sur 8 la proba est de 100% et 5 fois sur 8 elle est de 50%...

Toutefois, on note dans ce calcul que selon les années cette valeur oscille entre 50% et 100%, ce qui laisse penser que la probabilité d'observer un évènement de type **TS** ou **HU** est dépendant du nombre total de système dans l'année. Ce qui nous amène à la seconde option.

**Cette seconde option** consistera à chercher la distribution de Y la catégorie de l'évènement en utilisant à nouveau Bayes, en posant Z la probabilité d'avoir z évènements de type cyclonique par an. Ainsi, $P(Y=y)$ peut se définir comme $P(Y=y) = P(Z=z) * /frac{P(Y=y|Z)}{P(Z=z|Y=y)}$ avec $P(Y=y|Z=z)$ la probabilité d'observer **y** évènements de type **TS** ou **HU** sachant qu'il y a **z** évènements et $P(Z=z)$ la probabilité d'observer **z** évènements. On sait que $P(Y=y|Z=z)$ est nulle si y /> z,

```{r TP4.3.3}

# P(Z) : période d'analyse 2000 - 2021
Z <- DT[Year >= 2000, list(N = length(Key)), by = list(Year)]
# La série est incomplète : on ajoute les années sans système
Z <- setorderv(rbind(data.table(Year=2000:2021,N=0)[!Year%in%(Z$Year)], Z),"Year")
# mean(Z$N)
# sd(Z$N)
# A priori mu est différent de sigma donc on privilégirait une loi Binomiale Négative
descdist(Z$N, discrete=T, boot = 100)

fit.poisson <- fitdist(Z$N, "pois");summary(fit.poisson)
fit.negbin <- fitdist(Z$N, "nbinom");summary(fit.negbin)
fit.norm <- fitdist(Z$N, "norm", discrete = T);summary(fit.norm)
# Après execution : l'AIC le plus faible est celui de la loi de Poisson que l'on retiendra donc

fit.Z <- fit.poisson
fit.Z$estimate

# Le nombre d'évènement (tous types confondus) par an pourrait suivre une loi de Poisson.

ggplot(Y, aes(x = p, fill = as.factor(N))) + 
  geom_density(alpha=0.4) + scale_fill_grey() + theme_classic() +
  labs(fill="Nombre annuel de système") +
  theme(legend.position="bottom")

# Il semble compliqué d'établir une loi par nombre annuel d'évènement sur aussi peu d'historique... 
Y_est_N <- Y[,list(Y_est = sum(N_y)/sum(N), Y_sd = sd(p)),by = list(n=N)]
# Avec ces paramètres il reste possible d'effectuer des tirages aléatoires selon des lois uniformes, normales, binomiales négatives ou de Poisson.

# NB : que faire si Z > 3 ?
dpois(4:10,fit.Z$estimate)
# environ 5% des tirages poserons question
```

A ce stade, nous pouvons définir une probabilité conditionnelle mais uniquement pour les années ayant 1 à 3 systèmes. Si Z />= 4 alors nous sommes coincés. Une approche par la théorie de la crédibilité pourrait être testée, cela semble complexe à développer en cours. Nous avons toutefois une distribution classique de Z telle que $Z ~~suit~ Poisson(/lambda)$.

A partir de lÃÂ , nous disposons donc d'un loi de distribution de Z et de Y et de Y sachant Z, les lois de Z sachant Y et de Y sachant X sont équivalentes à des indicatrices. Par conséquent, si nous ajustons la loi de X sachant Y, nous pourrons alors remonter à la loi de X, le plus simple étant d'opter pour une approche par simulation.

```{r TP4.3.4}

# Cette étape est la même que la 4.2 toutefois elle s'applique sur toutes les données

# Distribution de la vitesse du vent max annuel 
descdist(DT$Wind, boot = 100)

maxi <- max(DT$Wind, 180)
mini <- min(DT$Wind, 0)
xscaled <- (DT$Wind-mini)/maxi
fit.beta <- fitdist(xscaled, "beta",  method = "mle")
plot(fit.beta); title("BETA")
summary(fit.beta)

fit.gev <- extRemes::fevd(DT$Wind,type="GEV",threshold=35)
plot(fit.gev); title("GEV")
summary(fit.gev)

fit.gpd <- extRemes::fevd(DT$Wind,type="GP",threshold=35)
plot(fit.gpd); title("GPD")
summary(fit.gpd)
```

Il ressort que la **loi Beta semble plus adaptée** à cette modélisation, on fera cependant attention aux ordre de grandeur de l'AIC, les données étant transformée pour l'ajustement de la loi Beta. Nous noterons que la distribution de Pareto Généralisée conserve les même défauts que précédemment, notamment sur la queue de distribution. Concernant l'approche par GEV, elle démontre des difficultés à expliquer les vents extrêmes. Nous noterons que le graphe des période de retour place les points forts dans le bas de l'intervalle de confiance. En restant prudent, cela peut venir de la stationnarité des données, le changement climatique engendrant des écarts entre l'historique lointain et les données récentes.

```{r TP4.3.5, message=FALSE, warning=FALSE}

# Nous ajouterons à la probabilité P(Z>0) : 1-ppois(0, fit.Z$estimate)

Zajust <- 1-ppois(0, fit.Z$estimate)

# Etant donné que nous travaillons sur une loi en combinant 2, il ne sera pas possible de passer par le quantile, on va donc écrire des équations à résoudre

f.beta <- function(x) {
  y <- 1 / (Zajust*(1 - pbeta(q = x, fit.beta$estimate[1], fit.beta$estimate[2])))
  return(y)
}

f.gev <- function(x) {
  y <- 1 / (Zajust*(1 - extRemes::pevd(x, threshold = 35, loc = fit.gev$results$par[1], scale = fit.gev$results$par[2], shape = fit.gev$results$par[3], type = "GEV")))
  return(y)
}

f.gpd <- function(x) {
  y <- 1 / (Zajust*(1 - extRemes::pevd(x, threshold = 35, scale = fit.gpd$results$par[1], shape = fit.gpd$results$par[2], type = "GP")))
  return(y)
}

inverse = function(fn, interval = NULL, lower = min(interval), upper = max(interval), ...){
    Vectorize(function(y){
        uniroot(f=function(x){fn(x)-y}, lower=lower, upper=upper, ...)$root
    })
}

f.betainv = inverse(f.beta, lower = 0, upper =250)
f.gevinv = inverse(f.gev, lower = 0, upper =250)
f.gpdinv = inverse(f.gpd, lower = 0, upper =250)

ReturnPeriod2 <- data.table(rbind(
  data.frame(law = "Beta", PdR_100ans = round(maxi*f.betainv(100)+mini,digits = 1),
             PdR_200ans = round(maxi*f.betainv(200)+mini,digits = 1),
             PdR_max = round(f.beta((max(DT$Wind)-mini)/maxi),digits = 0),
             PdR_cat3 = round(f.beta((96-mini)/maxi),digits = 0),
             PdR_cat4 = round(f.beta((113-mini)/maxi),digits = 0),
             PdR_cat5 = round(f.beta((137-mini)/maxi),digits = 0)),
  data.frame(law = "GEV", PdR_100ans = round(f.gevinv(100),digits = 1),
             PdR_200ans = round(f.gevinv(200),digits = 1),
             PdR_max = round(f.gev(max(DT$Wind)),digits = 0),
             PdR_cat3 = round(f.gev(96),digits = 0),
             PdR_cat4 = round(f.gev(113),digits = 0),
             PdR_cat5 = round(f.gev(137),digits = 0)),
  data.frame(law = "GPD", PdR_100ans = round(f.gpdinv(100),digits = 1),
             PdR_200ans = round(f.gpdinv(200),digits = 1),
             PdR_max = round(f.gpd(max(DT$Wind)),digits = 0),
             PdR_cat3 = round(f.gpd(96),digits = 0),
             PdR_cat4 = round(f.gpd(113),digits = 0),
             PdR_cat5 = round(f.gpd(137),digits = 0))))

ReturnPeriod2
```

## Conclusion

Nous avons pu proposer 2 méthodes pour estimer les périodes de retour possible pour les ouragans dans les Antilles Françaises. Chaque approche et chaque modèle ayant ses forces et faiblesses. En comparant ces résultats avec les publications disponibles il ressort que nos résultats ne sont pas si loin que ça de ce que d'autres spécialistes ont publiés. On notera toutefois que l'incertitude bien que faible peut avoir un impact fort sur les pertes.

La prochaine étape, maintenant que nous avons une estimation sur l'aléa (le rique Ouragan) consiste à estimer les pertes associées à ces évènements.
